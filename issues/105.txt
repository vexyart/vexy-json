Analyze the entire `./llms.txt` codebase snapshot. 

Our code, `vexy_json` is a Rust port of the `jsonic` parser written in JS. Neither of the tolerant and forgiving JSON-esque parsers (our `vexy_json` or the original `jsonic`) can overcome the problem of mismatching brackets or braces. 

It would be very useful if this were possible. 

Research this topic using all tools you can. 

<research1>
Great. I’ll investigate theoretical approaches, practical parser implementations, and advanced recovery techniques—across multiple languages including Rust, JS, and Python—that enable tolerant JSON-like parsers to recover from mismatched brackets or braces.

I'll let you know as soon as I have a comprehensive summary of relevant strategies and technologies.


# Advanced Techniques for Tolerant JSON Parsing and Error Recovery

Modern parsers increasingly aim to **gracefully handle syntax errors** so that a parse can continue and produce a partial result (with error diagnostics) rather than bailing out on the first failure. For JSON and similar data languages, a common challenge is **mismatched or unbalanced brackets/braces** – e.g. missing a closing `}` or having an extra `]`. Below, we survey theoretical foundations and practical strategies for tolerant parsing, focusing on how parsers can recover from such errors while constructing as much of the AST as possible.

## Theoretical Foundations of Error Recovery

**Classic Error Recovery:** Traditional compiler textbooks describe techniques like *panic-mode recovery* and *phrase-level recovery*. In panic mode (used in LL/LR parsers), the parser on encountering an error will **skip tokens until a synchronizing token** (such as a `}` or `]` or `;`) is found, then resume parsing. Phrase-level recovery involves adding special *error productions* to the grammar – for example, productions that match a common error pattern – so the parser can consume the bad input as an “error token” and continue. These approaches localize the damage but often at the cost of discarding some input.

**Error Tokens in LR Parsing:** LR parser generators (like Yacc/Bison) let grammar rules include the reserved symbol `error`. For instance, one can write a rule for an object like: `Object : '{' KV_Pairs '}' | '{' KV_Pairs error '}'` – meaning if something goes wrong in the middle, jump to after the error and continue at the closing brace. This is essentially a declarative form of panic-mode: the parser will synchronize on the `}` token and resume. This approach can handle mismatched braces by consuming stray tokens until a `}` is seen, but it requires anticipating error points in the grammar. As Matklad notes, trying to encode every possible error case in the grammar isn’t practical – *“you don’t want to anticipate every possible error, as there are just too many possibilities”*. A more flexible approach is needed for full resilience.

**LL vs GLR for Resilience:** Recent work in parser design emphasizes *never giving up on parsing*. Matklad explains that a top-down LL parser can be made resilient simply by *not crashing on the first error*, instead inserting a placeholder and moving on. This tends to naturally allow incomplete structures (e.g. an unclosed `{ ...` will be treated as an object with a missing `}`) to be represented. In contrast, a GLR parser (like Tree-sitter) will **explore many parse paths** and can often find valid substructures even if the overall input is broken. Tree-sitter’s generalized LR algorithm will produce an **`ERROR` node** in the syntax tree when it encounters an unexpected token, and then try to continue parsing thereafter. This means it can preserve well-formed portions of the input as subtrees. The downside is that GLR might not always assemble these fragments into the intended higher-level structure if the enclosing delimiters are incomplete. Still, GLR-based tools (Tree-sitter, Menhir’s GLR mode, etc.) are highly effective at parsing *through* errors to provide an AST with error nodes.

**PEG Parsers with Labeled Failures:** A notable advance in parsing theory is applying error recovery to PEG (Parsing Expression Grammars). Standard PEGs normally fail on invalid syntax, but researchers (Medeiros, Mascarenhas, etc.) introduced *PEGs with labeled failures and recovery expressions*. In this approach, grammar rules are augmented with *error labels* and custom recovery expressions that activate when a specific failure occurs. For example, a JSON PEG rule for an object might be extended to catch a missing `}` and invoke a recovery that skips forward to the next `}`. **The key principles** of this method are: **(1)** *Parsing never truly fails* – the parser always returns *some* syntax tree (with errors noted), treating a failure as just another outcome. **(2)** The grammar is extended with *embedded recovery rules* (annotated with labels) which, when a failure is encountered, will **emit an error message but then consume or insert tokens as needed to continue**. **(3)** The use of *synchronization tokens* is crucial: special terminal symbols like `}`, `]`, `)`, `;` (depending on the language) are used in recovery rules as anchor points to resume parsing beyond a malformed construct. This ensures that, say, an unterminated array `[...]` will recover by skipping ahead to the `]` (or end-of-file) so that subsequent JSON elements can still be parsed without cascade errors. These ideas mirror long-standing practices in hand-written parsers, but bringing them into PEG form means even automatically-generated parsers can benefit.

> **Example – Labeled PEG Recovery:** In a 2018 study, researchers built a PEG parser for Lua that *never fails*. They added error labels for expected tokens and provided recovery rules. For a missing `)` error, the recovery might be as simple as an *epsilon* (i.e. pretend the `)` was there, but record an error). For a missing `}` error, a custom rule might *skip to the next `}`* while accounting for nested braces. This skip rule sacrifices the content of the broken block but avoids flooding the user with spurious errors for each token inside that block. They also compute FOLLOW sets to create default recoveries – e.g. if an expression in a JSON-like grammar fails, skip tokens until a `,` or `}` (whatever would legally follow that expression). This is analogous to panic-mode but done in-line in the PEG grammar. The result was a JSON/Lua parser that *produces an AST 100% of the time* and reports errors with good accuracy.

**Minimal Edit Distance Repairs:** In academic literature, there are also algorithmic approaches to *automatically correct* mismatched delimiters with minimal changes. One classic strategy uses dynamic programming to find the smallest set of insertions/deletions needed to balance parentheses. A naive DP runs in cubic time, but recent algorithms solve it more efficiently for “almost correct” inputs. For example, Backurs and Onak (2016) present a linear-time algorithm when the number of edits *d* is small. They explicitly note that *“fixing parentheses sequences is related to repairing semi-structured documents such as XML and JSON.”*. While such global repair algorithms guarantee an optimal fix (in terms of edit count), they are complex to implement and not typically used in mainstream JSON parsers. However, the underlying idea – *balance the brackets by minimal adjustments* – inspires simpler heuristics in practice (see below).

## Strategies in Practice for Mismatched Brackets

Real-world tolerant parsers often implement **bespoke heuristics** to handle unbalanced or mis-nested braces:

* **Balance and Correct Brackets Heuristically:** A good example is the Rust crate **`json_repair`**, which provides a function specifically to *repair JSON bracket errors*. Its strategy is straightforward but effective:

  * **Skip extra closing brackets** that have no matching opener (for instance, if a `}` appears but there’s no open `{` in scope, it’s discarded as an error).
  * **Insert missing closers** at end-of-input for any still-open `{` or `[`.
  * **Replace mismatched closers** with the expected type (e.g. if the parser expected `]` but found `}`, treat it as `]` to close the current array, and record that as a fix).
  * Importantly, this function does *not* try to infer missing commas or quotes – it focuses only on structural delimiters. By doing so, it ensures the JSON’s high-level structure becomes balanced, making it valid enough to be parsed normally in a second stage. These kinds of fixes (dropping extra tokens, inserting needed ones) are analogous to single-token deletion/insertion in compiler error recovery.

* **Panic-Mode with Delimiter Awareness:** Many JSON parsers, when encountering a syntax error *inside* a nested structure, will jump to the end of that structure to regain footing. For example, V8’s JSON parser (in Chrome) on a bad token might scan ahead for a `,` or closing bracket at the same nesting level to resume. In a tolerant JSON parser, if we’re inside an array and hit a malformed element, we can **skip tokens until we see a `]` or `}` that matches some open container**, then continue from there. This avoids compounding one small error into many. The IntelliJ parser framework employs this idea: it *“pins”* a parse state when an opening bracket is read (committing to a certain construct), and if an error occurs, it *“drops tokens until a sentinel (like the matching closing bracket) is found”* before resuming【41†L42-L Fifty】. This matches common sense for JSON: once you’ve started an array or object, if something goes wrong inside, jump to after the would-be closing delimiter to start parsing the next element or section.

* **Nested Delimiter Recovery in Combinator Parsers:** Libraries like Rust’s **Chumsky** provide out-of-the-box recovery combinators. For instance, Chumsky has a `nested_delimiters(start, end, recovery)` strategy that will attempt to skip forward from a `{` or `[` to its matching `}` or `]`, keeping track of nested pairs, and insert an `Error` node if something went wrong in between. There is also a more general `skip_until([tokens])` which can drop input until one of a set of tokens is seen. In practice, a tolerant JSON parser using Chumsky could attach a recovery to object and array parsers such that any failure triggers a skip-until `}` or `]`. Chumsky’s author notes that enabling error recovery is often only a few lines of code – *“the code required to perform error recovery in \[the Chumsky] JSON parser is just 3 lines”*. This typically involves calling `.recover_with(skip_until([']','}']))` or similar on the parser for a value.

* **Pegged JSON Parsers (JSON5/JSONC):** JSON5 and JSONC are extensions of JSON that allow trailing commas, comments, and other leniencies. The parsers for these (e.g. Microsoft’s **`jsonc-parser`** for VSCode) are built to **not choke on minor errors**. The `jsonc-parser` will, for example, treat a trailing comma or missing quote as an error in an internal error list, but still produce a parse tree or JavaScript object for the content. According to its documentation, *“on invalid input, the parser tries to be as fault tolerant as possible, but still return a result”*. Similarly, Python’s **`tolerantjson`** library advertises parsing JSON in a *“best-effort”* way: it can handle single-quoted strings, trailing commas, etc., reporting errors but producing an output structure. For an input like `[1,2,{"a":"apple",}]` (which has an illegal trailing comma), tolerant parsers will typically ignore the trailing comma and successfully return the array `[1,2,{"a": "apple"}]`. In doing so, they effectively *remove or skip unneeded commas or braces*. These libraries often provide callbacks or error lists so the caller knows a correction was made (for instance, that a comma was dropped or a quote was inserted).

* **Custom Recovery in Hand-Written Parsers:** Many robust JSON parsers are hand-written (or use lower-level parser generators) to implement precise recovery logic. For example, the JSON parser in Firefox’s DevTools will try to auto-insert missing quotes for keys or wrap bare words in quotes, etc. In Rust’s Serde JSON, strict parsing is the default (it errors on any spec violation), but one could wrap it with a preprocessor that fixes common bracket mistakes using the `json_repair` approach above. Another approach is to parse in a streaming fashion (token by token) and *manually handle errors*: e.g. keep a stack of `{` and `[`; when a closing bracket arrives, check if it matches the top of stack – if not, either adjust it or ignore it. This way, by the end of input, any still-open brackets can be closed automatically. Such logic can be implemented in a state machine or recursive descent parser without too much complexity.

* **Multiple Error Reporting:** Tolerant parsers also strive to report *all* the issues they find, rather than stopping at the first. This means continuing the parse even after a fix. As Joshua Barretto points out, *“a parser should be able to encounter bad syntax, **generate an error**, and then continue onward to find more errors”*. For JSON this might mean you discover a missing comma, report it, insert one, and keep parsing the next element. To achieve this, the parser’s internal loops must catch errors and recover, instead of unwinding completely. Libraries like Chumsky and the approach by Eyal Kalderon with `nom` use this principle: they accumulate errors in a list while still returning a syntax tree. The final output is something like `(PartialAST, [Error])` rather than a thrown exception or null result.

## Partial AST Construction with Error Nodes

A critical aspect of tolerant parsing is designing the AST (abstract syntax tree) or parse tree to **accommodate errors**. This typically means introducing an **“error node” or “error value”** in the tree. For example, a JSON AST might represent an invalid value as an `ErrorValue` node (or simply as `null` with an attached error). This placeholder occupies the spot in the tree so that the structure of the rest of the document is preserved. As Barretto notes, *“the parser should preserve the information it has discovered… and pass \[a partial syntax tree] forward”* even in the face of errors. Many modern language parsers (Rust’s libsyntax, Microsoft’s Roslyn for C#, etc.) follow this philosophy: rather than losing the subtree after an error, they insert a dummy node. For instance, if a `}` is missing, the AST might include an `ErrorNode` in place of the missing brace, but the following code’s AST will attach to the correct parent node, so the compiler or IDE can still reason about it.

For JSON, a **full recovery** might mean the parser *inserts all missing brackets and quotes* needed to make the JSON formally valid. An example is the **Toy JSON Formatter** crate in Rust, which aims for *“extreme fault-tolerance”* such that *“any input should still yield a parsed JSON value regardless of how malformed it is,”* while *also yielding an error to signal that the input was transformed*. In practice, this means if you give it `{ key: "value" ` (missing a `}` and missing quotes on the key), it might produce a parsed value equivalent to `{"key": "value"}` and report that it inserted a quote and a brace. The AST here contains the object with a key node `"key"` and value `"value"`, and perhaps an attached error about the fixes. The ability to produce a valid JSON output (or AST) after repairs is useful for applications like auto-correcting linters or visualization tools that want to “heal” JSON.

**IDE Example:** In an editor, if you have `{ "foo": 123, "bar": [ 10, 20, 30 }` (where the array and object braces don’t match), a tolerant parser in a language server would likely:

1. Create an AST node for object `{…}` and an AST node for the array `[ … ]` for `"bar"`.
2. When it sees the `}` instead of the expected `]`, it can decide to treat `}` as closing the array (perhaps with an error like "Mismatched bracket, ']' expected"). It would close the array node, then see that the object is also now closed (since the same `}` could serve as the object’s closer). In effect, one token served two closures – the array’s `]` was “missing” and the `}` was consumed for it.
3. The parse continues after the object. The AST now has the `"bar"` key mapped to an array node that is marked incomplete/error, but the `"foo":123` entry remains intact, and the overall AST is a top-level object node (rather than nothing).
4. The IDE can highlight the error at the position of `}` and possibly at end-of-file (saying one brace was inserted implicitly). But crucially, it can still allow navigation of the AST (e.g. the user can fold the object, see both keys, etc.).

To implement similar features in a **Rust-based tolerant JSON parser** (like extending `vexy_json` or `jsonic` with mismatch recovery), one can combine these approaches:

* **Insert error nodes** on-the-fly in the parser: e.g. if expecting `]` but got `}`, push an error and treat it as the end of the array. If end-of-file reached but stack isn’t empty, auto-insert the needed `]` or `}` tokens (recording errors for each).
* **Use a recovery combinator or manual check** after a failed parse: e.g. if a value parser fails inside an array, skip forward until a `]` or `}` is found, or until a comma that indicates the next element, then continue. This could be done by a loop scanning the char stream or by a higher-level library feature.
* **Leverage existing crates**: As noted, libraries like Chumsky can simplify this – you can wrap sub-parsers with `.recover_with(nested_delimiters('[', ']', ...))` for arrays and similarly for `{`/`}` for objects. This built-in will automatically handle skipping over nested brackets until the matching closer is found. If using a parser-generator like LALRPOP or Menhir, you might simulate this by writing an error rule that matches any sequence of tokens until a `]` is seen.
* **Testing and tuning with examples**: It’s important to test various malformed inputs (extra closing braces, missing multiple closings, mis-ordered closures) to ensure the recovery logic doesn’t, for instance, *consume too much*. Sometimes a naive panic-mode will skip a `}` that actually was the one needed to close the object, leading to cascading confusion. Tools like the PEG recovery paper suggest using FOLLOW set guidance to stop skipping right *before* the next valid construct. In practice, this might mean if you’re parsing a value and it fails, you skip until you see **either** a comma **or** a closing brace/bracket at the current level, since those likely signal the end of the value. Implementing these sentinel checks (e.g., treat `}` or `]` as a signal to stop skipping) will improve the quality of recovery.

## Conclusion and Further Reading

Error-tolerant parsing is an active area of language tooling research and development. Techniques range from simple heuristics (skip to next `}`) to sophisticated grammar transformations that guarantee a parse for any input. **State-of-the-art academic work** (like PEG labeled failures or minimal edit algorithms) has started to find its way into practical tools, meaning library authors no longer have to choose between strict compliance and usability. Open-source libraries such as **Chumsky** (Rust) and **LPegLabel** (Lua) demonstrate that we can recover from errors while still using high-level parser definitions. And specialized JSON fixers like **`json_repair`**, **JSON5/JSONC parsers**, and **tolerantjson** for Python show concrete strategies to handle unbalanced delimiters, missing commas, and more in real JSON inputs.

For implementing a tolerant parser in Rust (as with `vexy_json` or similar), a combined approach is recommended:

* Use a **stack-based mechanism** or parser combinator to detect and auto-correct bracket mismatches in one pass.
* Represent the parse result in a way that **retains malformed parts** (e.g. an `Enum Value { Valid(...), Error(...) }`), so that no information is lost.
* Provide the user of the parser with an **error list or callback** indicating what was fixed or skipped, much like `serde_json::from_str` returns an error, but here we’d return a result plus a log of recoveries.

By studying robust parsers (like those in compilers or the JSONC parser) and perhaps integrating algorithms from academic research, one can extend a JSON parser to be highly tolerant. The end result is a parser that **“does not accept invalid JSON, yet never fails to produce a syntax tree”** – instead, it accepts superset syntax or auto-corrects JSON in a well-defined way, enabling applications to work with imperfect data while still warning about the issues.

**Sources:**

* Matklad, *Resilient LL Parsing Tutorial* – explains error resilience in hand-written parsers.
* Barretto, *Why Can’t Error-Tolerant Parsers Be Easy to Write?* – discusses modern requirements for parsers, including multiple error reporting and partial ASTs.
* Kalderon, *Error Recovery in nom (Rust)* – demonstrates adapting PEG error-recovery techniques (labels, sync tokens) to parser combinators.
* **json-repair** crate documentation – outlines a strategy to balance and fix JSON brackets.
* **jsonc-parser** (VSCode) – a fault-tolerant JSON parser for JSON with comments.
* **toy-json-formatter** crate – aims for extreme fault tolerance in JSON parsing.
* **tolerantjson** (Python) – example of a tolerant JSON parser handling common errors.
* Backurs & Onak, *Fast Algorithms for Parsing Parentheses with Few Errors* – academic approach to optimal bracket repair.
</research1>

Into `./issues/106.txt` write an extensive spec that will describe in minute detail step-by-step how to incorporate https://docs.rs/json-repair/ into our project. 

The general idea is that it should be an option, and we should have a parser that parses using the fastest method first, then falls back to our core `vexy_json` slower one, and only if that fails falls back to `json-repair`. 

